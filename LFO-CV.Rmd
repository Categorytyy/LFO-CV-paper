---
title: "Approximate leave-future-out cross-validation for time series models"
author: "Paul-Christian BÃ¼rkner, Jonah Gabry, Aki Vehtari"
abstract: |
  One of the common goals of a time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model's future predictive accuracy, for instance, for the purpose of model comparison or selection. As exact cross-validation for Bayesian models is often computationally expensive, approximate cross-validation methods have been developed; most notably methods for leave-one-out cross-validation (LOO-CV).
  Although LOO-CV can be used for time series model assessment, if the actual prediction task is to predict future given the past, LOO-CV provides optimistic estimate as the information from the future observations is available to influence predictions of the past.
  To properly take into account the prediction task and time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto-smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational cost while also providing informative diagnostics about the quality of the approximation.
keywords: keywords
lang: en-US
class: man
# figsintext: true
numbersections: true
encoding: UTF-8
bibliography: LFO-CV.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: false
header-includes:
   - \usepackage{amsmath}
   - \usepackage[utf8]{inputenc}
   - \usepackage[T1]{fontenc}
   - \usepackage{setspace}
   - \onehalfspacing
   - \setcitestyle{round}
   - \newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
editor_options: 
  chunk_output_type: console
---

```{r setup, cache = FALSE, include = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
options(knitr.kable.NA = '')
```

```{r packages, cache = FALSE, include = FALSE}
library(knitr)
library(kableExtra)
library(latex2exp)
library(tidyverse)
library(brms)
library(loo)
source("sim_functions.R")

# set ggplot theme
theme_set(bayesplot::theme_default())
colors <- unname(unlist(bayesplot::color_scheme_get()[c(6, 2)]))

# set rstan options
rstan::rstan_options(auto_write = TRUE)
options(mc.cores = max(1, parallel::detectCores() - 1))
```

```{r functions}
fmt <- function(x, digits = 1, ...) {
  format(x, digits = digits, nsmall = digits, ...)
}
```

# Introduction

A time series is a set of observations each one being recorded at a specific
time [@brockwell2002]. In statistics, a wide range of time series models has
been developed, which find application in nearly all empirical sciences [e.g.,
see @brockwell2002; @hamilton1994]. One common goal of a time series
analysis is to use the observed series to inform predictions for future.
When working with discrete time -- in which time points form
discrete set -- we will refer to the task of predicting a sequence of $M$ future
observations as $M$-step-ahead prediction ($M$-SAP). Once we have
fit a Bayesian model and can sample from the posterior predictive distribution,
it is straightforward to generate predictions as far into the future as we want.
It is also straightforward to evaluate the $M$-SAP performance of a time series
model by comparing the predictions to the observed sequence of $M$ future data
points once they become available.

It is common that we would like to estimate the future predictive performance
*before* we can collect the future observations.
If we have many competing models we may also need to
first decide which of the models (or which combination of the models) we should
rely on for predictions [@geisser1979; @hoeting1999; @vehtari2002; @ando2010; 
@vehtari2012]. In the absence of new data with which to evaluate predictive
performance, one general approach for evaluating a model's predictive accuracy
is cross-validation [@vehtari2002]. When doing cross-validation, the data is
split into two subsets. Based on the first subset we fit the statistical model
and then evaluate its predictive accuracy for the second subset. We may do this
once or many times, each time leaving out another subset.

If there were no time ordering in the data or if the focus is to assess the
non-time-dependent part of the model, we could use methods like leave-one-out
cross-validation (LOO-CV). For a data set with $N$ observations, we refit the
model $N$ times, each time leaving out one of the $N$ observations and assessing
how well the model predicts the left-out observation. LOO-CV is often computationally
expensive, but the Pareto smoothed importance
sampling [PSIS; @vehtari2017loo; @vehtari2017psis] algorithm allows for
approximating exact LOO-CV with PSIS-LOO-CV. PSIS-LOO-CV requires only a single
fit of the full model and comes with diagnostics for assessing the validity of
the approximation.

LOO-CV is fine for time series model assessment if we are interested in the performance  of interpolation within time series or the local conditional distribution. If we are interested instead in the performance of extrapolation to new future time points, leave out observations one at a time allows information from the future to influence predictions of the
past (i.e., times $t + 1, t+2, \ldots$ should not be used to predict for time
$t$). To apply the idea of cross-validation to the $M$-SAP case, we need some form
of leave-*future*-out cross-validation (LFO-CV). LFO-CV
does not refer to one particular prediction task but rather to various possible
cross-validation approaches that all involve some form of prediction for future time. Like exact LOO-CV, exact LFO-CV requires refitting the model
many times to different subsets of the data, which is computationally
costly for most nontrivial examples, in particular for Bayesian inference where
refitting the model means estimating a new posterior distribution rather than a
point estimate.

% Although PSIS-LOO-CV provides an efficient approximation to exact LOO-CV, until
% now there has not been an analogous approximation to exact LFO-CV that
% drastically reduces the computational burden while also providing informative
% diagnostics about the quality of the approximation.
In this paper, we extend the ideas from PSIS-LOO-CV and 
present PSIS-LFO-CV, an algorithm that typically only requires refitting the
time-series model a small number times and will make LFO-CV tractable for many
more realistic applications than previously possible.

The structure of the paper is as follows. In Section \ref{m-sap} we introduce
the idea and various forms of $M$-step-ahead predictions and how to approximate
it using PSIS. In Section \ref{simulations}, we evaluate the accuracy of the
approximation using extensive simulations, before we provide two real world case
studies about the change of level of Lake Huron and the day of the cherry
blossoms in Japan across the years in Section \ref{case-studies}. We end with a
discussion of the usefulness and limitations of the approach in Section
\ref{discussion}.

# $M$-step-ahead predictions {#m-sap}

Assume we have a time series of observations $y = (y_1, y_2, \ldots, y_N)$ 
and let $L$ be the _minimum_ number of observations from the series that
we will require before making predictions for future data. Depending on the
application and how informative the data are, it may not be possible to make
reasonable predictions for $y_{i}$ based on $(y_1, \dots, y_{i-1})$ until $i$ is
large enough so that we can learn enough about the time series to predict future
observations. Setting $L=10$, for example, means that we will only assess
predictive performance starting with observation $y_{11}$, so that we
always have at least 10 previous observations to condition on.

In order to assess $M$-SAP performance we would like to compute the 
predictive densities

\begin{equation}
p(y_{i<M} \,|\, y_{<i}) = p(y_i, \ldots, y_{i + M - 1} \,|\, y_{1},...,y_{i-1}) 
\end{equation}

for each $i \in \{L + 1, \ldots, N - M + 1\}$, where we use 
$y_{i<M} = (y_i, \ldots, y_{i + M - 1})$ and $y_{<i} = (y_{1}, \ldots, y_{i-1})$ 
to shorten the notation. As a global measure of predictive accuracy, we
can use the expected log posterior density (ELPD), which, for LFO-CV, can
be defined as follows:

\begin{equation}
{\rm ELPD} = \sum_{i=L+1}^{N - M + 1} \log p(y_{i<M} \,|\, y_{<i})
\end{equation}

The quantities $p(y_{i<M} \,|\, y_{<i})$ can be computed with the help of the
posterior distribution $p(\theta \,|\, y_{<i})$ of the parameters $\theta$
conditional on only the first $i-1$ observations of the time-series:

\begin{equation}
p(y_{i<M} \,| \, y_{<i}) = 
  \int p(y_{i<M} \,| \, y_{<i}, \theta) \, p(\theta\,|\,y_{<i}) \,d\theta. 
\end{equation}

Having obtained $S$ draws $(\theta_{<i}^{(1)}, \ldots, \theta_{<i}^{(S)})$ 
from the posterior distribution $p(\theta\,|\,y_{<i})$, we can estimate 
$p(y_{i<M} | y_{<i})$ as

\begin{equation}
p(y_{i<M} \,|\, y_{<i}) \approx \frac{1}{S}
\sum_{s=1}^S p(y_{i<M} \,|\, y_{<i}, \theta_{<i}^{(s)}).
\end{equation}

If we deal with factorizable models in which the response values are
conditionally independent given the parameters, the likelihood can be written in
the familiar form

\begin{equation}
p(y \,|\, \theta) = \prod_{n=1}^N p(y_j \,|\, \theta).
\end{equation}

In this case, $p(y_{i<M} \,|\, y_{<i}, \theta_{<i})$ reduces to 
\begin{equation}
p(y_{i<M} \,|\, \theta_{<i}) = \prod_{n = i}^{i + M -1} p(y_j \,|\, \theta_{<i}),
\end{equation}
due to the assumption of conditional independence between $y_{i<M}$ and $y_{<i}$
given $\theta_{<i}$. Non-factorizable models, which do not make this assumption,
are discussed in more detail in @buerkner:non-factorizable.

## Approximate $M$-step-ahead predictions {#approximate_MSAP}

The above equations include the posterior distributions from many
different fits of the model to different subsets of the data. To obtain
the predictive density $p(y_{i<M} \,|\, y_{<i})$ a model is fit to
only the first $i-1$ data points, and we will need to do this for every value of
$i$ under consideration (all $i \in \{L + 1, \ldots, N - M + 1\}$).

Below, we will present a new algorithm to reduce the number of models that need
to be fit for the purpose of obtaining each of the densities $p(y_{i<M} \,|\,
y_{<i})$. This algorithm relies in a central manner on Pareto smoothed
importance sampling [@vehtari2017loo; @vehtari2017psis], which we will to
introduce first.

### Pareto smoothed importance sampling {#psis}

In general, importance sampling is a technique to compute expectations with
respect to some target distribution using an approximating proposal distribution
that is easier to draw samples from than the actual target. If $p(\theta)$ is
the target and $g(\theta)$ is the proposal distribution, we can write any
expectation $\mathbb{E}_p[h(\theta)]$ of some function $h(\theta)$ with
respect to the target $p$ as

\begin{equation}
\mathbb{E}_p[h(\theta)] = \int h(\theta) p(\theta) \,d\, \theta 
 = \int \frac{[h(\theta) p(\theta) / g(\theta)] g(\theta)}
    {[p(\theta) / g(\theta)] g(\theta)} \,d\, \theta 
 = \int \frac{h(\theta) r(\theta) g(\theta)}
    {r(\theta) g(\theta)} \,d\, \theta 
\end{equation}

with importance ratios 
\begin{equation}
r(\theta) = \frac{p(\theta)}{g(\theta)}.
\end{equation}

Accordingly, if $\theta^{(s)}$ are $S$ random draws from $g(\theta)$, we can
approximate

\begin{equation}
\mathbb{E}_p[h(\theta)] \approx 
\frac{\sum_{s=1}^S h(\theta^{(s)}) r(\theta^{(s)})}{\sum_{s=1}^S r(\theta^{(s)})}
\end{equation}

provided that we can compute the raw importance ratios $r(\theta^{(s)})$ up to
some multiplicative constant. The main problem with this approach is that
the raw importance ratios tend to have high or even infinite variance and
as such, results computed on their basis can be highly unstable. 

In order to stablize those computations, one solution is to regularize the
largest raw importance ratios using the corresponding quantiles of generalized
Pareto distribution fitted to the importance ratios. This procedure is called
Pareto smooth importance sampling [PSIS; @vehtari2017loo; @vehtari2017psis] and
turned out to be superior to other commonly used reqularization technigues
[@vehtari2017psis]. In addition, PSIS comes with a useful diagnostic to evaluate
the goodness of the importance sampling approximation. The shape parameter $k$
of the generalized Pareto distribution provides information about the number of
existing moments with the variance existing for $k < 0.5$ and the mean existing
for $k < 1$. That is, if $k < 0.5$, the PSIS weights are highly stable and the
approximation can generally be trusted [@vehtari2017psis]. In practice, PSIS 
turned out to be relatively robust for $k < 0.7$ [@vehtari2017loo; 
@vehtari2017psis]. As such, the default threshold is set to $0.7$
when performing PSIS LOO-CV [@vehtari2017loo].


### PSIS applied to $M$-step-ahead predictions

We now come back to our task of performing $M$-step-ahead predictions in
time-series models. Starting with $i = N - M + 1$, we approximate each
$p(y_{i<M} \,|\, y_{<i})$ via

\begin{equation}
 p(y_{i<M} \,|\, y_{<i}) \approx
   \frac{ \sum_{s=1}^S w_i^{(s)}\, p(y_{i<M} \,|\, \theta^{(s)})}{ \sum_{s=1}^S w_i^{(s)}},
\end{equation}

where $w_i^{(s)}$ are the PSIS weights and $\theta^{(s)}$ are draws from the
posterior distribution based on _all_ observations. To obtain $w_i^{(s)}$, we
first compute the raw importance ratios

\begin{equation}
r_i^{(s)} \propto \frac{1}{\prod_{j \in J_i} p(y_j \,|\, \,\theta^{(s)})},
\end{equation}

and then stabilize them using PSIS as described above. The index set $J_i$
contains all the indices of observations which are part of the actually fitted
model but not of the model whose predictive performance we are trying to
approximate. That is, for the starting value $i = N - M + 1$, we have $J_i =
\{i, \ldots, N\}$. This approach to computing importance ratios is a
generalization of the approach used in PSIS-LOO-CV, where only a single
observation is left out at a time and thus $J_i = i$ for all $i$.

Starting from $i = N - M + 1$, we gradually *decrease* $i$ by $1$ (i.e., we move
backwards in time) and repeat the process. At some observation $i$, the
variability of importance ratios $r_i^{(s)}$ will become too large and
importance sampling fails. We will refer to this particular value of $i$ as
$i^\star_1$. To identify the value of $i^\star_1$, we check for which value of
$i$ does the estimated shape parameter $k$ of the generalized Pareto
distribution first cross a certain threshold $\tau$ [@vehtari2017psis]. Only
then do we refit the model using only observations before $i^\star_1$ and then
restart the process. Until the next refit, we have $J_i = \{i, \ldots, i^\star_1
-1 \}$ for $i < i^\star_1$, as the refitted model only contains the observations
up to index $N^\star_1 = i^\star_1 - 1$. An illustration of the above described procedure is shown in Figure \@ref(fig:vis-msap).

In some cases we may only need to refit once and in other cases we will find a
value $i^\star_2$ that requires a second refitting, maybe an $i^\star_3$ that
requires a third refitting, and so on. We repeat the refitting as few times as
is required (only if $k > \tau$) until we arrive at $i = L + 1$. Recall that $L$
is the minimum number of observations we have deemed acceptable for making
predictions (setting $L=0$ means predicting only based on the prior).

```{r vis-msap, fig.width=8, fig.height=3, fig.cap="Visualisation of PSIS approximated one-step-ahead predictions leaving out all future values. Predicted observations are indicated by **X**. In the shown example, the model was last refit at the $i^\\star = 5$th observation."}
status_levels <- c("included", "left out", "left out (PSIS)")
df <- data.frame(
  obs = rep(1:9, 3),
  i = factor(rep(3:5, each = 9)),
  Status = c(
    rep("included", 2), rep("left out (PSIS)", 2), rep("left out", 5),
    rep("included", 3), rep("left out (PSIS)", 1), rep("left out", 5),
    rep("included", 4), rep("left out", 5)
  )
) %>%
  mutate(Status = factor(Status, levels = status_levels))

msap_colors <- c(
  bayesplot::color_scheme_get("viridis")$light,
  bayesplot::color_scheme_get("viridis")$dark,
  bayesplot::color_scheme_get("viridis")$mid_highlight
)

ggplot(df, aes(obs, i, fill = Status)) +
  geom_tile(height = 0.9, width = 1, col = "black") +
  annotate(
    'text', x = 3:5, y = c(1, 2, 3), 
    label = "X", parse = TRUE, 
    size = 10, color = "white"
  ) +
  labs(x = "Observation", y = "Predicted observation") +
  scale_x_continuous(breaks = 1:9) +
  scale_fill_manual(values = msap_colors) +
  bayesplot::theme_default() +
  NULL
```

The threshold $\tau$ is crucial to the accuracy and speed of the proposed
algorithm. If $\tau$ is too large, we need fewer refits and thus achieve higher
speed, but accuracy is likely to suffer. If $\tau$ is too small, we get high
accuracy but a lot of refits to that speed will drop noticeably. When performing
exact CV of Bayesian models, almost all of the computational time is spend
fitting models, while the time needed to do predictions is negligible in
comparison. That is, a reduction of the number of refits basically implies a
proportional reduction in the overall time necessary for CV of Bayesian models.

When diagnostic Pareto-$k<0.5$ importance sampling estimate has finite variance, and based on the central limit theorem the convergence of the estimate is fast and then approximate CV via PSIS is highly accurate
[@vehtari2017psis]. For \(0.5 \leq k < 1\), generalized central limit theorem holds, but the convergence rate drops quickly when $k$ increases [@vehtari2017psis].
In practice, PSIS-LOO-CV  has practically useful error rates for 
$k < 0.7$ [@vehtari2017loo]. For PSIS-LFO-CV introduced in the
present paper, we can expect an appropriate threshold to be somewhere between
$0.5 \leq \tau \leq 0.7$. It is unlikely to be as high as $\tau = 0.7$, as the
errors are not independent. If there is a large error when leaving out $i$th
observation, , then there is likely to be a large error when leaving out observations
$i-1, i-2, \ldots$ until a refit is performed. That
is, highly influential observations with high $k$ are likely to have stronger effects
for total estimate in LFO-CV than LOO-CV. We will come back to the issue of setting appropriate
thresholds in Section \ref{simulations}.

## Block $M$-step-ahead predictions {#approximate_blockMSAP}

Depending on the particular time-series data and model, the Pareto $k$ estimates
may exceed $\tau$ rather quickly (i.e., after only few observations) and so
a lot of refits may be required even when carrying out the PSIS approximation
to LFO-CV. In this case, another option is to exclude only the block of $B$ 
future values that directly follow the observations to be predicted while 
retaining all of the more distant values $y_{i>B} = (y_{i + B}, \ldots, y_N)$. 
This will usually result in lower Pareto $k$ estimates and thus less refitting,
but crucially alters the underlying prediction task, to which we will refer
to as block-$M$-SAP.

The block-$M$-SAP version closely resembles the basic $M$-SAP only if values in
the distant future, $y_{>B}$, contain little information about the current
observations being predicted, apart from just increasing precision of the 
estimated global parameters. Whether this assumption is justified will depend
on the data and model. That is, if the time-series is non-stationary, distant
future value will inform overall trends in the data and thus clearly inform
predictions of the current observations being left-out. As a result, 
block-LFO-CV is only recommended for stationary time-series and corresponding
models.

There are more complexities that arise in block-$M$-SAP that we did not have to
care about in standard $M$-SAP. One is that, by just removing the block, the
time-series effectively gets split into two parts, one before and one after the
block. This poses no problem for conditionally independent time-series models,
where predictions just depend on the parameters and not on the former values of
the time-series itself. However, if the model's predictions are *not*
conditionally independent as is the case, for instance, in autoregressive models
(see Section \ref{simulations}), the observations of the left-out block have to
be modeled as missing values in order to retain the integrity of the
time-series' predictions after the block. A related example from spatial
statistics, in which the modeling of missing values is required for valid
inference, can be found in @buerkner:non-factorizable.

Another complexity concerns the PSIS approximation of block-LFO-CV: Not only
does the approximating model contain more observations than the current model
whose predictions we are approximating, but it also may *not* contain
observations that are present in the actual model. The latter observations are
those right after the currently left-out block, which are included in the
current model, but not in the approximating model as they were part of the block
at the time the approximating model was (re-)fit. A visualisation of this
situation is provided in Figure X. More formally, let $\overline{J}_i$ be the
index set of observations that are missing in the approximating model at the
time of predicting observation $i$. We find

\begin{equation}
\overline{J}_i = \{ \max(i + B, N^\star + 1), \ldots, \min(N^\star + B, N) \}
\end{equation}

if $\max(i + B, N^\star + 1) \leq \min(N^\star + B, N)$ and 
$\overline{J}_i = \emptyset$ otherwise. As above, $N^\star$ refers to the 
largest observation included in the model fitting, that is 
$N^\star = i^\star - 1$ where $i^*$ is the index of the latest refit. The raw 
importance ratios $r_i^{(s)}$ for each posterior draw $s$ are then computed as

\begin{equation}
r_i^{(s)} \propto \frac{\prod_{j \in \overline{J}_i} p(y_j \,|\, \,\theta^{(s)})}
{\prod_{j \in J_i} p(y_j \,|\, \,\theta^{(s)})}
\end{equation}

before they are stabilized and further processed using PSIS (see Section
\ref{approximate_MSAP}).

```{r vis-block-msap, fig.width=8, fig.height=3, fig.cap="Visualisation of PSIS approximated one-step-ahead predictions leaving out a block of $B = 3$ future values. Predicted observations are indicated by **X**. Observation in the left out block are indicated by **B**. In the shown example, the model was last refit at the $i^\\star = 5$th observation."}
status_levels <- c("included", "included (PSIS)", "left out", "left out (PSIS)")
df <- data.frame(
  obs = rep(1:9, 3),
  i = factor(rep(3:5, each = 9)),
  Status = c(
    rep("included", 2), rep("left out (PSIS)", 2), rep("left out", 2), 
    rep("included (PSIS)", 2), rep("included", 1),
    rep("included", 3), rep("left out (PSIS)", 1), rep("left out", 3), 
    rep("included (PSIS)", 1), rep("included", 1),
    rep("included", 4), rep("left out", 4), rep("included", 1)
  )
) %>%
  mutate(Status = factor(Status, levels = status_levels))

block_msap_colors <- c(
  bayesplot::color_scheme_get("viridis")$light,
  bayesplot::color_scheme_get("viridis")$light_highlight,
  bayesplot::color_scheme_get("viridis")$dark,
  bayesplot::color_scheme_get("viridis")$mid_highlight
)

ggplot(df, aes(obs, i, fill = Status)) +
  geom_tile(height = 0.9, width = 1, col = "black") +
  annotate(
    'text', x = 3:5, y = 1:3, 
    label = "X", parse = TRUE, 
    size = 10, color = "white"
  ) +
  annotate(
    'text', x = c(4:6, 5:7, 6:8), y = rep(1:3, each = 3), 
    label = "B", parse = TRUE, size = 10, color = "white"
  ) +
  labs(x = "Observation", y = "Predicted observation") +
  scale_x_continuous(breaks = 1:9) +
  scale_fill_manual(values = block_msap_colors) +
  bayesplot::theme_default() +
  NULL
```

# Simulations {#simulations}

To evaluate the goodness of the approximation of PSIS-LFO-CV,
we performed a simulation study by systematically varying the following
conditions: The number $M$ of future observations to be predicted took on 
values of $M = 1$ and $M = 4$. The number of future values to be excluded in the model
fitting took on values of $B = \infty$ (i.e., leaving out the whole future), or
$B = 10$ (i.e. leaving out only a block of 10 observations).
The threshold $\tau$ of the Pareto $k$ estimates was varied between 
$k = 0.5$ to $k = 0.7$ in steps of $0.1$. In addition, we evaluated six 
different data generating models with linear and/or quadratic terms and/or
autoregressive terms of order 2 (see Table X for an overview). These models are
also illustrated graphically in Figure \ref{fig:simmodels}. In all conditions,
the time-series consistent of $N = 200$ observations and the minimal number
of observations to make predictions was set to $L = 25$ throughout.

```{r, include = FALSE}
seed <- 1234
set.seed(seed)
N <- 200
time <- seq_len(N)
stime <- scale_unit_interval(time)
models <- c(
  "constant", "linear", "quadratic",
  "AR2-only", "AR2-linear", "AR2-quadratic"
)

fits <- preds <- setNames(vector("list", length(models)), models)
for (m in names(fits)) {
   file <- paste0("models/fit_", m)
   fits[[m]] <- fit_model(model = m, N = N, seed = seed, file = file)
   pred <- posterior_predict(fits[[m]])
   preds[[m]] <- fits[[m]]$data %>% 
     mutate(       
       time = time, 
       stime = stime,
       Estimate = colMeans(pred), 
       Q5 = apply(pred, 2, quantile, probs = 0.05),
       Q95 = apply(pred, 2, quantile, probs = 0.95),
       model = m
    )
}
preds <- as_tibble(bind_rows(preds)) %>%
   mutate(model = factor(model, levels = models)) %>%
   select(-`I(stime^2)`)
```

```{r simmodels, fig.height=4, fig.cap="Illustration of the models used in the simulations."}
ggplot(preds, aes(x = time, y = Estimate)) +
  facet_wrap(~model) +
  geom_smooth(aes(ymin = Q5, ymax = Q95), stat = "identity", size = 0.5) +
  geom_point(aes(y = y), size = 0.5) +
  labs(y = "y")
```

Autoregressive (AR) models are some of the most commonly used time-series models. 
An AR(p) model -- an autoregressive model of order $p$ -- can be defined as

\begin{equation}
y_i = \eta_i + \sum_{k = 1}^p \varphi_k y_{i - k} + \varepsilon_i,
\end{equation}

where $\eta_i$ is the linear predictor for the $i$th observation, $\phi_k$ are
the autoregressive parameters and $\varepsilon_i$ are pairwise independent
errors, which are usually assumed to be normally distributed with equal variance
$\sigma^2$. The model implies a recursive formula that allows for computing the
right-hand side of the above equation for observation $i$ based on the values of
the equations for previous observations. Thus, by definition, responses of
AR-models are not conditionally independent. However they are still
factorizable, that is we may write down a separate likelihood contribution per
observation [see @buerkner:non-factorizable for more discussion on 
factorizability of statistical models].

All simulations were done in R [@R2018] using the brms package [@brms1;
@brms2] together with the probabilistic programming language Stan
[@carpenter2017] for the modeling fitting, the loo package [@vehtari2017loo] for
the PSIS computation, and several tidyverse packages [@tidyverse] for data
processing. The full code used in the simulations as well as all results are
available on Github (https://github.com/paul-buerkner/LFO-CV-paper).

## Results {#sim_results}

```{r}
mlevels <- c(
  "constant", "linear", "quadratic",
  "AR2-only", "AR2-linear", "AR2-quadratic"
)
tau_levels <- TeX(paste0("$\\tau$ = ", c(0.5, 0.6, 0.7)))
lfo_sims <- read_rds("results/lfo_sims.rds") %>%
  as_tibble() %>%
  mutate(
    model = factor(model, levels = mlevels),
    tau = factor(k_thres, labels = tau_levels),
    elpd_loo = map_dbl(res, ~ .$loo_cv$estimates["elpd_loo", 1]),
    elpd_exact_lfo = map_dbl(res, ~ .$lfo_exact_elpd[1]),
    elpd_approx_lfo = map_dbl(res, ~ .$lfo_approx_elpd[1]),
    elpd_diff_lfo = elpd_approx_lfo - elpd_exact_lfo,
    elpd_diff_loo = elpd_loo - elpd_exact_lfo,
    npreds = map_dbl(res, ~ sum(!is.na(.$lfo_approx_elpds))),
    nrefits = lengths(map(res, ~ attr(.$lfo_approx_elpds, "refits"))),
    rel_nrefits = nrefits / npreds
  )

# create subsets
lfo_sims_1sap <- lfo_sims %>% filter(is.na(B), M == 1)
block_lfo_sims_1sap <- lfo_sims %>% filter(!is.na(B), M == 1)
lfo_sims_4sap <- lfo_sims %>% filter(is.na(B), M == 4)
block_lfo_sims_4sap <- lfo_sims %>% filter(!is.na(B), M == 4)
```

Results of the 1-SAP simulations leaving out all future values are visualized
in Figure \ref{fig:1sap}. Comparing the columns of Figure \ref{fig:1sap}, it
is clearly visible that the accuracy of the PSIS approximation increases with
decreasing $\tau$, up to almost perfect accuracy for $\tau = 0.5$. At the same 
time, the percentage of observations at which refitting the model was required
increased substantially with decreasing $\tau$ (see Table 
\ref{tab:refits}). Using $\tau = 0.6$ induced a slight positive
bias in PSIS-LFO-CV, but also reduced the number of required refits by roughly 
$30\%$. Another $30\%$ reduction in the number of refits was achieved by
using $\tau = 0.7$ but at the cost of disproportionally increasing the 
positive bias in PSIS-LFO-CV. As predicted by theory, LOO-CV is a biased
estimate of 1-SAP performance when leaving out all future values for all
non-constant models in particular those with a trend in the time-series (see
light-blue histograms in Figure \ref{fig:1sap}).

```{r 1sap, fig.height=8, fig.cap="Simulation results of 1-step-ahead predictions."}
lfo_sims_1sap %>% 
  select(elpd_diff_lfo, elpd_diff_loo, model, tau) %>%
  gather("Type", "elpd_diff", elpd_diff_lfo, elpd_diff_loo) %>%
  ggplot(aes(x = elpd_diff, y = ..density.., fill = Type)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7) +
  scale_fill_manual(
    values = colors,
    labels = c("Approximate LFO-CV", "Approximate LOO-CV")
  ) +
  labs(x = 'Difference to exact LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

```{r refits, cache=FALSE}
lfo_sims %>% 
  select(model, B, M, k_thres, rel_nrefits) %>%
  mutate(B = ifelse(is.na(B), "$\\infty$", B)) %>%
  group_by(model, B, M, k_thres,) %>%
  summarise(rel_nrefits = round(mean(rel_nrefits), 2)) %>%
  ungroup() %>%
  spread("model", "rel_nrefits") %>%
  group_by(B) %>%
  mutate(M = ifelse(duplicated(M), "", M)) %>%
  ungroup() %>%
  mutate(B = ifelse(duplicated(B), "", B)) %>%
  rename(`$\\tau$` = "k_thres") %>%
  kable(
    caption = "Mean percentages of required refits.",
    booktabs = TRUE,
    escape = FALSE
  ) %>%
  footnote(
    general = "Note: Results are based on 100 simulation trials of time-series with $N = 200$ observations requiring at least $L = 25$ observations to make predictions. Abbreviations: $\\\\tau$ = threshold of the Pareto-k-estimates; M = number of predicted future observations; B = number of left-out future observations.",
    general_title = "",
    threeparttable = TRUE,
    escape = FALSE
  )
```

Results of the 4-SAP simulations leaving out all future values are visualized in
Figure \ref{fig:4sap}. Comparing the columns of Figure \ref{fig:4sap}, it is
clearly visible that the accuracy of the PSIS approximation increases with
decreasing $\tau$, up to almost perfect accuracy for $\tau = 0.5$. At the same
time, the percentage of observations at which refitting the model was required
increased substantially with decreasing $\tau$ (see Table \ref{tab:refits}). In
light of the corresponding 1-SAP results (see above), this is not surprising as
the procedure to determining the necessity of a refit is independent of $M$ (see
Section \ref{approximate_MSAP}). Using $\tau = 0.6$ again induced a slight
positive bias in PSIS-LFO-CV, but also reduced the number of required refits by
roughly $30\%$. Another $30\%$ reduction in the number of refits was achieved by
using $\tau = 0.7$ but at the cost of disproportionally increasing the positive
bias in PSIS-LFO-CV. PSIS-LOO-CV is not displayed in Figure \ref{fig:4sap} as
the number of observations predicted as each step (4 vs. 1) renders 4-SAP LFO-CV
and LOO-CV incomparable.

```{r 4sap, fig.height=8, fig.cap="Simulation results of 4-step-ahead predictions."}
lfo_sims_4sap %>% 
  select(elpd_diff_lfo, model, tau) %>%
  ggplot(aes(x = elpd_diff_lfo, y = ..density..)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7, fill = colors[1]) +
  labs(x = 'Difference to exact LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

Results of the block-1-SAP simulations leaving out a block of $B = 10$ future
values are shown in Figure \ref{fig:block1sap}. PSIS-LFO-CV provides almost unbiased
estimate of the
corresponding exact LFO-CV for all investigated conditions, that is regardless
of the threshold $\tau$ or the data generating model. The number of required
refits was not only much smaller than when leaving out all future values, but
practically approached zero for most conditions. PSIS-LOO-CV has also small bias, but higher variance than PSIS-LFO-CV. This is plausible given that LOO-CV
and LFO-CV of block-1-SAP only differ in whether they include the relatively few
observations in the block when fitting the approximating model.

```{r block1sap, fig.height=8, fig.cap="Simulation results of block 1-step-ahead predictions."}
block_lfo_sims_1sap %>% 
  select(elpd_diff_lfo, elpd_diff_loo, model, tau) %>%
  gather("Type", "elpd_diff", elpd_diff_lfo, elpd_diff_loo) %>%
  ggplot(aes(x = elpd_diff, y = ..density.., fill = Type)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7) +
  scale_fill_manual(
    values = colors,
    labels = c("Approximate LFO-CV", "Approximate LOO-CV")
  ) +
  labs(x = 'Difference to exact block-LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

Results of the block-4-SAP simulations leaving out a block of $B = 10$ future
values (see Figure \ref{fig:block4sap}) are overall similar to the
corresponding 1-SAP simulations. In particular, PSIS-LFO-CV has small bias compared to
the exact LFO-CV. However, the accuracy of
PSIS-LFO-CV for block-4-SAP is highly variable when applied to
autoregressive models (see the last three rows in Figure \ref{fig:block4sap}),
something that is also visible in block-1-SAP although to a smaller degree. This
seems to be a counter-intuitive result given that predictions should be more
certain in the block version as more observations are available to inform the
model. However, it can be explained as follows. In autoregressive models,
predictions of future observations directly depend on past observations, that is
predictions are not conditionally independent. This becomes a problem when
dealing with observations that are missing in the approximating model right
after the block of left out observations, since the directly preceding
observations are part of the block and are thus have to be treated as missing
values (for details see Section \ref{approximate_blockMSAP}). This implies a
disproportionally high variability in the predictions of observations right
after the block in autoregressive models, which then naturally propagates into
higher variability of the PSIS-LFO-CV approximations.

```{r block4sap, fig.height=8, fig.cap="Simulation results of block 4-step-ahead predictions."}
block_lfo_sims_4sap %>% 
  select(elpd_diff_lfo, model, tau) %>%
  ggplot(aes(x = elpd_diff_lfo, y = ..density..)) +
  facet_grid(
    model ~ tau, scales = "free_y", 
    labeller = label_parsed
  ) +
  geom_histogram(alpha = 0.7, fill = colors[1]) +
  labs(x = 'Difference to exact block-LFO-CV', y = "Density") +
  geom_vline(xintercept = 0, linetype = 2) +
  theme_bw() +
  theme(legend.position = "bottom") +
  NULL
```

# Case Studies {#case-studies}

## Annual measurements of the level of Lake Huron

To illustrate the application of PSIS-LFO-CV for estimating expected $M$-SAP
performance, we will fit a model for 98 annual measurements of the water level
(in feet) of [Lake Huron](https://en.wikipedia.org/wiki/Lake_Huron) from the
years 1875--1972. This data set is found in the *datasets* R package, which is
installed automatically with R [@R2018]. The time-series shows rather strong
autocorrelation of the level as some trend towards lower levels for later points
in time. We fit an AR(4) model and display the model implied predictions along
with the observed values in Figure \ref{fig:lake-huron}.

```{r}
data("LakeHuron")
N <- length(LakeHuron)
df <- data.frame(
  y = as.numeric(LakeHuron),
  year = as.numeric(time(LakeHuron)),
  time = 1:N
) 
```

```{r fit_lh, results = "hide"}
fit_lh <- brm(
  y | mi() ~ 1, 
  data = df, 
  autocor = cor_ar(~time, p = 4), 
  prior = prior(normal(0, 0.5), class = "ar"),
  control = list(adapt_delta = 0.99), chains = 2,
   seed = 583829, file = "models/fit_lh"
)
```

```{r lake-huron, fig.cap="Water Level in Lake Huron (1875-1972). Black points are observed data. The blue line represents mean predictions of an AR(4) model with 90% prediction intervals shown in gray.", fig.height=3}
preds <- posterior_predict(fit_lh)
preds <- cbind(
  Estimate = colMeans(preds), 
  Q5 = apply(preds, 2, quantile, probs = 0.05),
  Q95 = apply(preds, 2, quantile, probs = 0.95)
)

ggplot(cbind(df, preds), aes(x = year, y = Estimate)) +
  geom_smooth(aes(ymin = Q5, ymax = Q95), stat = "identity", size = 0.5) +
  geom_point(aes(y = y)) + 
  labs(y = "Water Level (ft)", x = "Year")
```

```{r}
L <- 20
k_thres <- 0.6
loo_lh <- loo(log_lik(fit_lh)[, (L + 1):N])
```

Based on this data and model, we will illustrate the use of PSIS-LFO-CV to
provide estimates of $1$-SAP and $4$-SAP leaving out all future values as well
as leaving out only a block of future values. To allow for reasonable
predictions of future values, we will require at least $L = 20$ historical
observations (20 years) to make predictions. Further, we set a threshold of
$\tau =$ `r k_thres` for the Pareto $k$ value at which define that refitting 
becomes necessary. Our fully reproducible analysis of this case study can be 
found on GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r}
M <- 1
exact_lfo_1sap_lh <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, 
  file = "results/exact_lfo_1sap_lh.rds"
)
approx_lfo_1sap_lh <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, 
  file = "results/approx_lfo_1sap_lh.rds"
)
refits <- attributes(approx_lfo_1sap_lh)$refits
nrefits <- length(refits)

sum_exact_lfo_1sap_lh <- summarize_elpds(exact_lfo_1sap_lh)[1]
sum_approx_lfo_1sap_lh <- summarize_elpds(approx_lfo_1sap_lh)[1]
```

```{r}
M <- 4
exact_lfo_4sap_lh <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, 
  file = "results/exact_lfo_4sap_lh.rds"
)
approx_lfo_4sap_lh <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, 
  file = "results/approx_lfo_4sap_lh.rds"
)
sum_exact_lfo_4sap_lh <- summarize_elpds(exact_lfo_4sap_lh)[1]
sum_approx_lfo_4sap_lh <- summarize_elpds(approx_lfo_4sap_lh)[1]
```

We start by computing exact and PSIS-approximated LFO-CV of 1-SAP where we
leave out all future values. We compute 
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_exact_lfo_1sap_lh, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_approx_lfo_1sap_lh, 1)`, 
which are highly similar. The LOO-CV result
${\rm ELPD}_{\rm loo} =$ `r fmt(loo_lh$estimates[1, 1], 1)` tends to slighly
overestimate the predictive performance, which coincides with our simulation
results of stationary autoregessive models (see fourth row of Figure 
\@ref(fig:1sap)). Plotting the Pareto $k$ estimates reveals that the
model had to be refit `r nrefits` times, out of a total of $N - L =$ 
`r N - L` predicted observations (see Figure \ref{fig:pareto-k-lh}). On average, 
this means one refit every `r fmt((N - L) / nrefits, 1)` observations, which
implies a drastic speed increase as compared to exact LFO-CV.
Performing LFO-CV of 4-SAP leaving out all future values, we compute
${\rm ELPD}_{\rm exact} =$ `r fmt(sum_exact_lfo_4sap_lh, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(sum_approx_lfo_4sap_lh, 1)`, 
which are again highly similar. In general for increasing $M$, the approximation
tends to become less accurate in absolute ELPD units, as the ELPD increment of
each observation will be based on more and more observations. Since, for
constant threshold $\tau$, the importance weights are the same independent of
$M$, Pareto $k$ estimates are also the same in $4$-SAP as in $1$-SAP.

```{r pareto-k-lh, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the Lake Huron model leaving out all future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- attributes(approx_lfo_1sap_lh)$ks
ids <- N:(L + 1)
plot_ks(ks, ids)
```

```{r}
B <- 10
M <- 1
exact_lfo_block1sap_lh <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, B = B,
  file = "results/exact_lfo_block1sap_lh.rds"
)
approx_lfo_block1sap_lh <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, B = B,
  file = "results/approx_lfo_block1sap_lh.rds"
)

refits <- attributes(approx_lfo_block1sap_lh)$refits
nrefits <- length(refits)

elpd_exact_lfo_block1sap_lh <- summarize_elpds(exact_lfo_block1sap_lh)[1]
elpd_approx_lfo_block1sap_lh <- summarize_elpds(approx_lfo_block1sap_lh)[1]
```

```{r}
M <- 4
exact_lfo_block4sap_lh <- compute_lfo(
  fit_lh, type = "exact", M = M, L = L, B = B,
  file = "results/exact_lfo_block4sap_lh.rds"
)
approx_lfo_block4sap_lh <- compute_lfo(
  fit_lh, type = "approx", M = M, L = L, B = B,
  file = "results/approx_lfo_block4sap_lh.rds"
)
elpd_exact_lfo_block4sap_lh <- summarize_elpds(exact_lfo_block4sap_lh)[1]
elpd_approx_lfo_block4sap_lh <- summarize_elpds(approx_lfo_block4sap_lh)[1]
```

It is not entirely clear how stationary the time-series is as it may have a
slight negative trend across time. However, the AR(4) model we are using assumes
stationarity and it is appropriate to also use block-LFO-CV for this example, at
least for illustration. We choose to leave out a block of $B = 10$
future values as the dependency of an AR(4) model will not reach that far into
the future. That is, we will include all observations after this block when
re-fitting the model.

Approximate LFO-CV of block-1-SAP reveals 
${\rm ELPD}_{\rm exact} =$ `r fmt(elpd_exact_lfo_block1sap_lh, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(elpd_approx_lfo_block1sap_lh, 1)`, 
which are highly similar. Plotting the Pareto $k$ estimates reveals that the
model had to be refit `r nrefits` times, out of a total of $N - L =$ 
`r N - L` predicted observations (see Figure \ref{fig:pareto-k-lh-block}). On average, 
this means one refit every `r fmt((N - L) / nrefits, 1)` observations, which
again implies a drastic speed increase as compared to exact LFO-CV. What is more,
we needed even fewer refits than in non-block LFO-CV, an observation we already
made in our simulation in Section \ref{simulations}.
Performing LFO-CV of block-4-SAP, we compute
${\rm ELPD}_{\rm exact} =$ `r fmt(elpd_exact_lfo_block4sap_lh, 1)` and 
${\rm ELPD}_{\rm approx} =$ `r fmt(elpd_approx_lfo_block4sap_lh, 1)`, 
which are again similar but not quite a close as in the 1-SAP case.
Since AR-models fall in the class of conditionally dependent models, predicting
observations right after the left-out block may be quite difficult as shown
in Section \ref{simulations}. However, for the present data set, the
PSIS approximations of block-LFO-CV seem to have worked out just fine.

```{r pareto-k-lh-block, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the Lake Huron model leaving out a block of 10 future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- attributes(approx_lfo_block1sap_lh)$ks
ids <- N:(L + 1)
plot_ks(ks, ids)
```


## Annual date of the cherry blossoms in Japan

```{r}
cherry <- read.csv("data/cherry_blossoms.csv")
cherry_temp <- cherry[!is.na(cherry$temp), ]
cherry_doy <- cherry[!is.na(cherry$doy), ]
```

The cherry blossom in Japan is a famous natural phenomenon occuring once every
year during spring. As climate changes so does the annual date of the cherry
blossom [@aono2008; @aono2010]. The most complete reconstruction available to
date contains data between `r min(cherry$year)` AD and `r max(cherry$year)` AD
[@aono2008; @aono2010]. The data is freely available online
(http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/).

In this case study, we are going to predict the annual date of the cherry
blossom using a thin-plate regression spline [@wood2003] with a high number (40) 
of basis function to provide flexible non-linear smoothing of the time-series. A
visualisation of both the data and the fitted model in provided in Figure
\ref{fig:cherry-blossom}. While the time-series appears rather stable across
earlier centuries, with substantial variation across consecutive years, there
are some clearly visible trends in the data. In particular in more recent years,
the cherry blossom tended to happen much earlier than before, presumably as a
result of climate change [@aono2008; @aono2010].

Based on this data and model, we will illustrate the use of PSIS-LFO-CV to
provide estimates of $1$-SAP and $4$-SAP leaving out all future values. To allow
for reasonable predictions of future values, we will require at least $L = 100$
historical observations (100 years) to make predictions. Further, we set a
threshold of $\tau =$ `r k_thres` for the Pareto $k$ value at which define that
refitting becomes necessary. As the time series is not stationary, in particular
towards the end, we do not investigate the models performance in terms of
block-LFO-CV. Our fully reproducible analysis of this case study can be found on
GitHub (https://github.com/paul-buerkner/LFO-CV-paper).

```{r fit_cb}
fit_cb <- brm(
  formula = bf(doy ~ s(year, k = 40)),
  data = cherry_doy, chain = 2, seed = 583829, 
  control = list(adapt_delta = 0.99),
  file = "models/fit_cb"
)
```

```{r cherry-blossom, fig.height=3, fig.cap="Day of the cherry blossom in Japan (812-2015). Black points are observed data. The blue line represents mean predictions of a thin-plate spline model with 90% regression intervals shown in gray."}
me_cb <- marginal_effects(fit_cb)
plot(me_cb, points = TRUE, plot = FALSE)[[1]] +
  labs(x = "Year", y = "Day of cherry blossom")
```

```{r}
N <- NROW(cherry_doy)
L <- 100
M <- 1
exact_lfo_1sap_cb <- compute_lfo(
  fit_cb, type = "exact", M = M, L = L, 
  file = "results/exact_lfo_1sap_cb.rds"
)
approx_lfo_1sap_cb <- compute_lfo(
  fit_cb, type = "approx", M = M, L = L, 
  file = "results/approx_lfo_1sap_cb.rds"
)
refits <- attributes(approx_lfo_1sap_cb)$refits
nrefits <- length(refits)

sum_exact_lfo_1sap_cb <- summarize_elpds(exact_lfo_1sap_cb)[1]
sum_approx_lfo_1sap_cb <- summarize_elpds(approx_lfo_1sap_cb)[1]

loo_cb <- loo(fit_cb, newdata = cherry_doy[-seq_len(L), ])
```

```{r}
M <- 4
exact_lfo_4sap_cb <- compute_lfo(
  fit_cb, type = "exact", M = M, L = L, 
  file = "results/exact_lfo_4sap_cb.rds"
)
approx_lfo_4sap_cb <- compute_lfo(
  fit_cb, type = "approx", M = M, L = L, 
  file = "results/approx_lfo_4sap_cb.rds"
)

sum_exact_lfo_4sap_cb <- summarize_elpds(exact_lfo_4sap_cb)[1]
sum_approx_lfo_4sap_cb <- summarize_elpds(approx_lfo_4sap_cb)[1]
```

We start by computing exact and PSIS-approximated LFO-CV of 1-SAP where we leave
out all future values. We compute ${\rm ELPD}_{\rm exact} =$ 
`r fmt(sum_exact_lfo_1sap_cb, 1)` and ${\rm ELPD}_{\rm approx} =$ 
`r fmt(sum_approx_lfo_1sap_cb, 1)`, which are similar. In comparison,
${\rm ELPD}_{\rm exact} =$ `r fmt(loo_cb$estimates[1, 1], 2)` clearly 
overestimates the predictive performance. Plotting the 
Pareto $k$ estimates reveals that the model had to be refit `r nrefits` times,
out of a total of $N - L =$ `r N - L` predicted observations (see Figure
\ref{fig:pareto-k-lh}). On average, this means one refit every 
`r fmt((N - L) / nrefits, 1)` observations, which implies a drastic speed 
increase as compared to exact LFO-CV. Performing LFO-CV of 4-SAP leaving out all 
future values, we compute ${\rm ELPD}_{\rm exact} =$ `r fmt(sum_exact_lfo_4sap_cb, 1)` 
and ${\rm ELPD}_{\rm approx} =$ `r fmt(sum_approx_lfo_4sap_cb, 1)`, which are again
similar, but not as close as in the 1-SAP case. This coincides with our simulation
results which indicated an increasing variance of PSIS-LFO-CV for increasing $M$.
Since, for constant threshold
$\tau$, the importance weights are the same independent of $M$, Pareto $k$
estimates are also the same in $4$-SAP as in $1$-SAP.

```{r pareto-k-cb, fig.height=2.5, fig.cap="Pareto $k$ estimates for PSIS-LFO-CV of the cherry blossom model leaving out all future values. The dotted red line indicates the threshold at which the refitting was necessary."}
ks <- attributes(approx_lfo_1sap_cb)$ks
ids <- N:(L + 1)
plot_ks(ks, ids)
```


# Discussion {#discussion}

In the present paper, we proposed and evaluated a new method to approximate
cross-validation methods for time-series models, which we called PSIS-LFO-CV. It
follows the common task of time-series models to predict future values based
solely on past values. Within the set of such prediction tasks, we can choose
the number $M$ of future values to be predicted at a time and how much of the
future we leave out, either all future values (M-SAP) or only a block of more
recent future values (block-M-SAP).

For a set of common time-series models, we established via simulations that
PSIS-LFO-CV is almost unbiased approximation of exact LFO-CV if we choose
the threshold $\tau$ of the Pareto-k-estimates to be not larger than $\tau =
0.6$. As the number of required model refits, and thus the computational time,
increases with decreasing $\tau$, we currently see $\tau = 0.6$ as a good
default when performing PSIS-LFO-CV. This is noticeably smaller than the
recommended threshold for PSIS-LOO-CV of $\tau = 0.7$, because, in PSIS-LFO-CV,
the errors are dependent as highly influential observations also influence the approximation in the following iterations before refit, thus having a stronger influence on the
overall accuracy than in PSIS-LOO-CV.

Among other things, our simulations indicated that the accuracy of PSIS
approximated block-M-SAP is highly variable for conditionally dependent
models such as autoregressive models (see Section \ref{sim_results} for
details). Together with the fact that block-M-SAP is only theoretically
reasonable for stationary time series, as the future will always be informative
for non-stationary ones, this leaves PSIS approximated block-M-SAP in a
difficult spot. It appears to be a theoretically reasonable and empirically
accurate choice only for conditionally independent models fit to stationary
time-series. If the time-series is not too long and the corresponding model not
too complex, so that a few more refits are acceptable, it might thus be more
consistent and safe to just using PSIS-LFO-CV of M-SAP not trying to approximate
block-M-SAP at all.

Lastly, we want to briefly note that LFO-CV can also be used to compute marginal
likelihoods. Using basic rules of conditional probability, we can factorize the
log marginal likelihood as

\begin{equation}
\log p(y) = \sum_{i=1}^N \log p(y_i \,|\, y_{<i}).
\end{equation}

This is nothing else than the ELPD of 1-SAP if we set $L = 0$, that is if we
choose to predict *all* observations using their respective past (the very
first observation is only predicted from the prior). As such, marginal
likelihoods may be approximated using PSIS-LFO-CV. Although this approach is
unlikely to be more efficient than methods specialized to compute marginal
likelihoods such as bridge sampling [@meng1996; @meng2002; @gronau2017], it may
be a noteworthy options if, for some reason, other methods fail.

\newpage

<div id="refs"></div>

